# Local RAG Chatbot with PDF Ingestion

## Overview

This project implements a local Retrieval-Augmented Generation (RAG) chatbot that ingests PDF documents, extracts their content and metadata, and uses Langchain and LangGraph to create a conversational AI. The chatbot allows users to ask questions about the content of the ingested PDFs and receive relevant answers.

## Key Features

-   **PDF Ingestion:** Processes PDF documents to extract text content and metadata.
-   **Markdown Conversion:** Converts PDF content to Markdown format for easier processing.
-   **Metadata Extraction:** Extracts relevant metadata from PDFs to enhance the retrieval process.
-   **Local RAG Implementation:** Utilizes a local RAG setup, ensuring data privacy and control.
-   **Langchain Integration:** Leverages Langchain for document loading, text splitting, and vectorstore management.
-   **LangGraph Implementation:** Employs LangGraph to orchestrate the RAG pipeline and manage conversation flow.
-   **Chat History:** Maintains chat history for contextual conversations.

## File Structure

The project structure is organized as follows:

-   `src/`: Contains the source code for the chatbot.
    -   `frontend.py`: Implements the user interface for uploading PDF files and interacting with the chatbot.
    -   `helper.py`: Initializes logging and defines project paths.
    -   `utils/`: Contains utility modules.
        -   `file_ops.py`: Implements file loading and saving operations.
        -   `index_files.py`: Loads documents and their metadata.
        -   `process_files.py`: Processes uploaded PDFs, extracts text and metadata.
        -   `final_block_rag.py`: Defines data structures for graph state.
        -   `structure_output.py`: Defines the structure for citation objects.
-   `README.md`: This file, providing an overview of the project.
-   `.gitignore`: Specifies intentionally untracked files that Git should ignore.

## Modules Description

-   **`frontend.py`**: This script provides the user interface for interacting with the chatbot. It includes a function `cached_process_files` that handles the upload of PDF files, saves them locally, and calls the backend processing function (`process_uploaded_pdfs`).

-   **`helper.py`**: This script initializes the logger and defines important project paths, such as the project root directory.  It also appends the project root and parent directories to the system path.

-   **`utils/file_ops.py`**: This module provides utility functions for loading and saving text files, ensuring UTF-8 encoding.

-   **`utils/index_files.py`**: This module provides the `load_documents_with_metadata` function, which loads markdown files and their metadata from a specified folder structure, creating Langchain `Document` objects.

-   **`utils/process_files.py`**: This module includes the core function `process_uploaded_pdfs` that converts PDF files to markdown, extracts metadata, and saves the results. It uses a private (self-created) PDF extraction library to convert the PDF (Refer to `pdf_text_extractor`).

-   **`utils/final_block_rag.py`**: Defines the `GraphState` TypedDict, which represents the state of the LangGraph graph, including the question, enhanced question, chat history, documents, and answer.

-   **`utils/structure_output.py`**: Defines the `Citation` Pydantic model for structuring citation information, including document name, number, and page numbers.

## Setup and Installation

1.  **Clone the repository:**

    ```bash
    git clone <repository_url>
    cd <repository_directory>
    ```

2.  **Install dependencies:**

    ```bash
    # Use the `uv` package for installing requirements
    `uv sync` would automatically setup a virtual environment under .venv
    ```

    If you want to do it without `uv`, you will have to use `pip install`. Check for the requirements in `pyproject.toml`.

3.  **Environment Configuration**
    *You will have to create an account with Groq and Gemini. Store the API keys in a local keyvault*

## Usage

1.  **Run the `frontend.py` script:**

    ```bash
    streamlit run src/frontend.py
    ```

2.  **Upload PDF files:**

    Use the user interface to upload the PDF documents you want to ingest.

3.  **Interact with the chatbot:**

    Ask questions related to the content of the uploaded PDFs and receive answers generated by the RAG pipeline.

## CLI (local) — quick start

If you prefer running the ingestion + local RAG flow from the command line (no Streamlit UI), use the included CLI runner `src/cli_run.py`.

1.  Create / activate your Python environment (example using conda):

```zsh
# create env (optional)
conda create -n rag-py311 python=3.11 -y
conda activate rag-py311
```

2.  Install dependencies from `requirements.txt` into the active environment:

```zsh
/Users/<you>/miniforge/envs/rag-py311/bin/python -m pip install -r requirements.txt
```

3.  Configure credentials and models in your `.env` file (project root). Example:

```properties
# Google / Gemini embeddings key (or use ADC via GOOGLE_APPLICATION_CREDENTIALS)
GEMINI_API_KEY=your_gemini_api_key_here

# Groq API key and model (used for generation)
GROQ_API_KEY=your_groq_api_key_here
MODEL=meta-llama/llama-4-maverick-17b-128e-instruct

# Embedding model (Gemini embeddings example)
EMBED_MODEL=gemini-embedding-001

# Optional: override evaluator model (defaults to MODEL if unset)
# EVAL_MODEL=meta-llama/llama-4-maverick-17b-128e-instruct
```

4.  Run the CLI and point it at a PDF to ingest (it copies the PDF into the project's local store and runs the pipeline):

```zsh
/Users/<you>/miniforge/envs/rag-py311/bin/python src/cli_run.py --pdf path/to/your_doc.pdf
```

When the CLI finishes ingesting and building the index it starts a small REPL where you can type questions. Type `exit` or `quit` to stop.

### Session Cleanup Options

The CLI now provides automatic cleanup features:

- **On Startup**: You'll be prompted to delete session-specific folders (`local_store/` and `vector_store/`) before processing a new PDF. This is useful for starting fresh with a new document.
  - Answer `y` to delete existing session data
  - Answer `n` to keep existing session data
  - Use `--no-cleanup-prompt` flag to skip this prompt and keep existing data

- **On Exit**: When you type `exit` or `quit` (or press Ctrl+C), the CLI automatically deletes all temporary `vector_store_tmp_*` folders that were created during the session.

Example usage:
```zsh
# With cleanup prompt (default)
python src/cli_run.py --pdf path/to/your_doc.pdf

# Skip cleanup prompt
python src/cli_run.py --pdf path/to/your_doc.pdf --no-cleanup-prompt

# Force clean reindex (deletes vector store before processing)
python src/cli_run.py --pdf path/to/your_doc.pdf --force
```

Notes and troubleshooting
- If the embedding or LLM calls return authentication/permission errors, verify the corresponding API keys (GEMINI_API_KEY, GROQ_API_KEY) or configure Google ADC (`GOOGLE_APPLICATION_CREDENTIALS`) for Google embedding.
- The local Qdrant store is created under `vector_store/`. If another process has the on-disk store open you may see a lock/`AlreadyLocked` error — the CLI contains a fallback to create a temporary local store. These temporary `vector_store_tmp_*` folders are automatically cleaned up when you exit the CLI.
- Optional functionality:
    - BM25 retriever: install `rank_bm25` to enable BM25-based retrieval:

```zsh
/Users/<you>/miniforge/envs/rag-py311/bin/python -m pip install rank_bm25
```

    - Better PDF layout: consider `pymupdf_layout` or ensure `pymupdf` is installed in the same environment.

Recommended next steps
- Pin and freeze your working environment once stable (use `pip freeze > requirements.txt` or update `requirements.txt` in this repo).
- If you require concurrent access to the same vector store from multiple processes, run a Qdrant server (docker or cloud) instead of the local on-disk store.

## Dependencies

-   [Langchain](https://www.langchain.com/): A framework for developing applications powered by language models.
-   [LangGraph](https://python.langchain.com/docs/langgraph): A library for building LLM-powered graphs.
-   [Pymupdf](https://pymupdf.readthedocs.io/en/latest/): A library to programmatically access and manipulate PDF documents.
-   [Streamlit](https://streamlit.io/): An open-source app framework for Machine Learning and Data Science teams.
-   [Pydantic](https://docs.pydantic.dev/): Data validation and settings management using Python type annotations.

## Contributing

Contributions are welcome! Please feel free to submit pull requests or open issues to suggest improvements or report bugs.

## License

[Private License]
