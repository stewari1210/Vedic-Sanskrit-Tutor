# ============================================================
# EMBEDDING MODEL CONFIGURATION
# ============================================================
# Choose your embedding provider:
#   - "local-fast": BAAI/bge-small-en-v1.5 (fastest, 95% quality, 130MB, unlimited)
#   - "local-best": sentence-transformers/all-mpnet-base-v2 (best quality, 420MB, unlimited)
#   - "gemini": Google Gemini embeddings (cloud, requires API key, has quotas)
#
# Default: local-fast
EMBEDDING_PROVIDER=local-fast

# Only needed if EMBEDDING_PROVIDER=gemini
EMBED_MODEL=
GEMINI_API_KEY=

# Document chunking
CHUNK_SIZE=1024
CHUNK_OVERLAP=128

# Retrieval settings
# Number of document chunks to retrieve for each query
# WARNING: Total context must fit within model's token limit!
# With CHUNK_SIZE=1024: RETRIEVAL_K * 1024 + overhead < 6000 tokens for llama-3.1-8b
# Recommended: 4-5 for llama-3.1-8b, 8-10 for larger context models
# Formula: RETRIEVAL_K â‰¤ (model_token_limit - 1500) / CHUNK_SIZE
RETRIEVAL_K=4

# LLM Configuration
MODEL=
EVAL_MODEL=
TEMPERATURE=
TOP_P=
MAX_TOKENS=
MAX_RETRIES=
TIMEOUT=

# Chat configuration
CHAT_MEMORY_WINDOW=
TOPIC_CHANGE_WINDOW=
COLLECTION_NAME=

