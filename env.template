# ============================================================
# EMBEDDING MODEL CONFIGURATION
# ============================================================
# Choose your embedding provider:
#   - "local-fast": BAAI/bge-small-en-v1.5 (fastest, 95% quality, 130MB, unlimited)
#   - "local-best": sentence-transformers/all-mpnet-base-v2 (best quality, 420MB, unlimited)
#   - "gemini": Google Gemini embeddings (cloud, requires API key, has quotas)
#
# Default: local-fast
EMBEDDING_PROVIDER=local-fast

# Only needed if EMBEDDING_PROVIDER=gemini
EMBED_MODEL=
GEMINI_API_KEY=

# Document chunking
CHUNK_SIZE=1024
CHUNK_OVERLAP=128

# Retrieval settings
# Number of document chunks to retrieve for each query
# WARNING: Total context must fit within model's token limit!
# With CHUNK_SIZE=1024: RETRIEVAL_K * 1024 + overhead < 6000 tokens for llama-3.1-8b
# Recommended: 4-5 for llama-3.1-8b, 8-10 for larger context models
# Formula: RETRIEVAL_K â‰¤ (model_token_limit - 1500) / CHUNK_SIZE
RETRIEVAL_K=4

# Document expansion via proper noun association
# Number of additional docs to retrieve per proper noun (names, places, tribes)
# for better context. Set to 0 to disable expansion.
EXPANSION_DOCS=3

# ============================================================
# LOW-CONFIDENCE ANSWER HANDLING (REGENERATION)
# ============================================================
# When the evaluator gives a confidence score < 75% or -1 (not enough info), the system can:
#   - Refine (false): Use the same model to refine its answer (limited improvement ~10-20%)
#   - Regenerate (true): Use a superior model to regenerate from scratch (much better ~60-80%)
#
# Regeneration uses API calls, but only when needed (~20% of queries).
# This significantly improves answer quality for difficult questions.
#
# Default: true (enable regeneration)
USE_REGENERATION=true

# Provider for regeneration model
# Options: groq, gemini, ollama
# Default: groq
REGENERATION_PROVIDER=groq

# Model to use for regeneration (provider-specific)
# Groq options: llama-3.3-70b-versatile, llama-3.1-70b-versatile
# Gemini options:
#   - gemini-1.5-flash (RECOMMENDED: 15 RPM, 1M requests/day, 4M tokens/day)
#   - gemini-1.5-pro (Higher quality: 2 RPM, 50 requests/day, 10M tokens/day)
#   - gemini-2.0-flash-exp (Experimental: 15 RPM, 1.5K requests/day - LIMITED!)
# Ollama options: qwen2.5:32b, llama3.1:70b, etc.
# Default: llama-3.3-70b-versatile (for groq)
REGENERATION_MODEL=llama-3.3-70b-versatile

# Maximum regeneration attempts per query (SAFEGUARD against infinite loops)
# Prevents API exhaustion when documents don't contain enough information.
# If evaluator keeps returning low confidence, system will accept answer after this many attempts.
# Default: 2 (allows 1 initial + 2 regeneration attempts = 3 total generations)
MAX_REGENERATION_ATTEMPTS=2

# LLM Configuration
MODEL=
EVAL_MODEL=
TEMPERATURE=
TOP_P=
MAX_TOKENS=
MAX_RETRIES=
TIMEOUT=

# Chat configuration
CHAT_MEMORY_WINDOW=
TOPIC_CHANGE_WINDOW=
COLLECTION_NAME=

